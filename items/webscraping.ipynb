{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import new\n",
    "from unittest import result\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import csv\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this project i webscraped [abjjad](https://www.abjjad.com/) using the BeautifulSoup python package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_book_name =[]\n",
    "new_author =[]\n",
    "new_cover_url = []\n",
    "next_page_links =[]\n",
    "new_genres = []\n",
    "new_descriptions = []\n",
    "page_number = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we get access to the website using the requests package\n",
    "from the main page we collect the book name, author, cover url and the link to the book detail page\n",
    "due to the structure of the website(no main page that contains all the books i found it easier to proccessess every genre web page separately thereby but i had to change the link multiple time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "while page_number<20:\n",
    "\n",
    "    result = requests.get(f'https://www.abjjad.com/books/220759001/%D8%B1%D9%88%D8%A7%D9%8A%D8%A7%D8%AA-%D9%88%D9%82%D8%B5%D8%B5/{page_number}/')       \n",
    "    src = result.content\n",
    "\n",
    "\n",
    "    goodsoup = BeautifulSoup(src,\"lxml\")\n",
    "\n",
    "     ## book title\n",
    "    book_name = goodsoup.find_all(\"a\", {\"data-ga\":\"BookBadge_Title\"})\n",
    "    \n",
    "    ## author name\n",
    "    author = goodsoup.find_all('span',{\"class\":\"author\"})\n",
    " \n",
    "    cover_url = goodsoup.find_all(\"a\",{\"class\":\"img\"})\n",
    "\n",
    "    #saving the detail page link into an array to get acceses to more data \n",
    "    for i in range(len(book_name)):\n",
    "        next_page_links.append(\"https://www.abjjad.com\" + str(book_name[i]).split('href=\"')[1].lstrip().split('\">')[0]) \n",
    "\n",
    "    for i in range(len(book_name)):        \n",
    "        NremovedLinks = str(book_name[i]).split('>')[1].lstrip().split('<')[0]\n",
    "        NremovedR = NremovedLinks.replace('\\r', '')\n",
    "        new_book_name.append(NremovedR.replace('\\n',''))\n",
    "\n",
    "        AremovedLinks = str(author[i].text)\n",
    "        AremovedR = AremovedLinks.replace('\\r', '')\n",
    "        new_author.append(AremovedR.replace('\\n',''))\n",
    "        new_cover_url.append(str(cover_url[i]).split('src=\"')[1].lstrip().split('\"/>')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this loop gets the descriptions and genres from the book detail page  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for link in next_page_links:\n",
    "    \n",
    "    secondresult = requests.get(link)\n",
    "    secondsrc = secondresult.content\n",
    "    secondgoodsoup = BeautifulSoup(secondsrc,'lxml') \n",
    "    genres = secondgoodsoup.find(\"ul\", {\"itemprop\":\"genre\" })\n",
    "    output_text = \"\"\n",
    "    for item in genres.find_all('li'): \n",
    "        output_text += item.text+\"|\"\n",
    "       \n",
    "    new_genres.append(output_text)\n",
    "\n",
    "    desc = secondgoodsoup.find(\"span\" ,{\"itemprop\":\"description\" , \"class\":\"content\"})\n",
    "    DremovedLinks = str(desc).split('>')[1].lstrip().split('<')[0]\n",
    "    DremovedR = DremovedLinks.replace('\\r', '')\n",
    "    new_descriptions.append(DremovedR.replace('\\n',''))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asave the collected dataset to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [new_book_name,new_author,new_cover_url,new_genres,new_descriptions]\n",
    "expo = zip_longest(*file_list)\n",
    "with open(\"C:\\\\Users\\\\Lana\\\\Desktop\\\\Misk_DSI_capstone\\\\data\\\\abjjad.csv\" , \"w\" , encoding=\"utf-8-sig\") as myfile:\n",
    "    writerobj = csv.writer(myfile)\n",
    "    writerobj.writerow([\"Book_title\",\"Author\",\"Cover_url\",\"genres\",\"descriptions\"])\n",
    "    writerobj.writerows(expo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec48d00f4eac919b72449f86026bbc65783148329d27938b3416269b8744bb8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

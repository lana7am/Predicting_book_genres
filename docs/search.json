[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting book genres",
    "section": "",
    "text": "predicting book genre based on book summary"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Can we judge a book buy its summury/plot?\nthe problem: knowing what genres and subgenres a book falls under\naim: to automate and accelerate the process of labeling the genres of books\ngoal: build a high accuracy model that predicted the genre of a given book\nimputes: to build a precise multilabel genre prediction model to better organize and categorize my home library :P\n\n\nDataset\ni scraped the data from the abbjad website\nthe dataset has 4160 rows\n\n\n\nattribute\ndescription\n\n\n\n\nBook_title\ntitle of the book\n\n\nAuthor\nauthor name\n\n\ncover_url\nbook cover image url\n\n\ngenres\nlist of genres that a book falls under\n\n\ndescriptions\nbook description\n\n\ncover_name\nstring name of the cover image\n\n\ndescription_length\nthe word count of the description\n\n\ngenre_count\nnumber of genres that a book has\n\n\n\n\n\nAssumptions\nmy intial assumptions was that it’s easy to build a highly accurate model that can predict the genres of a given book\n\n\nThe steps you took to find a solution\nafter web scraping the data i cleaned the dataset and applied nlp preprocess steps such as :  - removing white spaces,numbers,spiecal characters and diacritics - removing stop words - applying feature extraction method (TF-IDF)\nbuilding the model:\nwhile researching for this project i’ve stumbled upon a lot of similar projects and resources but the majority only allowed a single genre per book and worked with a small set of basic genres. but with this project i wanted to work with a variety of genres and subgenres. thereby classifying my problem as a Multilabel Classification problem.\nthe resources for Multilabel Classification is scarse. but while reading a lot of documentation i stumbled upon the MultiLabelBinarizer package which makes Multilabel Classification easier to apply. i applied th MultiLabelBinarizer() on the genre feature which allowed the values to be vectorized. after that i’ve found that logistic regression and one-vs-rest works best.\nresources: - which features extraction method to choose? - How to Solve a Multi Class Classification Problem with Python? -video on Multi-Label Text Classification with Scikit-MultiLearn in Python\n\n\nconclusion\nit is possible to predict book genre based on the book plot/summary and we might be able to achieve a higher accuracy if we have a bigger dataset.\n\n\nOutlook for future development\nfor this project i hope to continue building on the model to increase accuracy and to continue developing the computer vision aspect of the project. i would also like to find websites with a larger book achive to webscrape and use.\n\n\nLimitations & problems with your solution\ncomputaional power  time managment"
  },
  {
    "objectID": "items/data_cleaning.html",
    "href": "items/data_cleaning.html",
    "title": "Predicting_book_genres",
    "section": "",
    "text": "first i read the file and removed the new lines from the genre colum at the same step then i cleaned the datab by:\n- Removing empty string in genre - Removing duplicates in the genre list - Removing white spaces from the author’s name\n#read the file and removing the /n and |\ndf = pd.read_csv(\"/Users/Lana/Desktop/Misk_DSI_capstone/data/data/abjjad.csv\",converters={'genres': lambda x: re.split('\\||\\n', x[1:-1])})\ndf['descriptions'].replace('\\n','', regex=True).replace('\\r','').replace(\"  \",\"\").replace(\"»\",\"\").replace(\"«\",\"\")\ndf['Book_title'].replace('\\n','', regex=True).replace('\\r','').replace(\"  \",\"\")\n\n##removing empty string \ndf['genres'] = df['genres'].apply(lambda x: [s for s in x if s])\n\n##removing duplicates in the genre list\ndf['genres'] = df['genres'].apply(lambda x: list(dict.fromkeys(x)))\n\n#removing white spaces from the author's name\ndf['Author'] = df['Author'].apply(lambda x :x.strip())\n\n#getting cove name from cover_url\ndf['cover_name'] = df['Cover_url'].apply(lambda x : x.split(\"pub/\",1)[1])\n\ndata cleaning on the description colum/feature\n\nRemove punctuations\nRemove extra whitespace\nRemove diacritics\nRemove arabic numbers\nRemove english numbers\nRemove stop words\n\n\n## Remove punctuations\ndf['descriptions'] = df['descriptions'].apply(lambda x: re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,«»،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', x) )\n\n## remove extra whitespace\ndf['descriptions'] = df['descriptions'].apply(lambda x: re.sub('\\s+', ' ', x))\n\ndef normalizeArabic(text):\n    text = text.strip()\n    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    noise = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    text = re.sub(noise, '', text)\n    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n    return araby.strip_tashkeel(text)\n\n\n##remove_diacritics \ndf['descriptions'] = df['descriptions'].apply(lambda x: normalizeArabic(str(x)))\n\n##remove arabic numbers\nArabic_numbers = ['٤','١','٢','٣','٥','٦','٧','٨','٩','٠']\nfor word in range(0, len(Arabic_numbers)):\n    df['descriptions'] = df['descriptions'].replace(Arabic_numbers[word], '') \n\n##remove english numbers\ndf['descriptions'] = df['descriptions'].apply(lambda x: re.sub(r'[0-9a-zA-Z]+',' ', x))\n\n#def to remove stop words\ndef remove_stopwords(text):\n    arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n    for word in arb_stopwords:\n        normalizeArabic(word)\n    no_stopword_text = [w for w in text.split() if not w in arb_stopwords]\n    return ' '.join(no_stopword_text)\n\n\n\n\n\n##stemming?\nchecking for null values\n\nprint(df.isna().sum()) # <-- after check i had 1 null in Book_title and 5 in descriptions\ndf = df.dropna(axis=0, subset=['descriptions' , 'Book_title'])\ndf.shape\n\nBook_title      1\nAuthor          0\nCover_url       0\ngenres          0\ndescriptions    0\ncover_name      0\ndtype: int64\n\n\n(4160, 6)\n\n\nfirst i wanted to see how many unique genres and subgenres i have and what they are\n\n##count all the genres\nall_genres = []\nfor row in df['genres']:\n    for j in range(len(row)):\n        all_genres.append(row[j])\n\n#remove duplicets\nall_genres_types = list(dict.fromkeys(all_genres))     \n\nprint(all_genres_types)\nprint(len(all_genres_types))\n\n['روايات وقصص', 'روايات رومانسية', 'روايات خيالية', 'روايات اجتماعية', 'روايات واقعية', 'كتب الأدب', 'أدب عالمي مترجم', 'روايات تاريخية', 'روايات فلسفية', 'روايات مترجمة', 'روايات دينية', 'روايات روحانية', 'قصص قصيرة', 'روايات مغامرات', 'روايات رعب', 'روايات فانتازيا', 'روايات نفسية', 'روايات غموض', 'روايات سياسية', 'روايات وطنية', 'روايات الحرب', 'روايات بوليسية', 'روايات خيال علمي', 'روايات ديستوبيا', 'روايات أدب السجون', 'روايات وقصص ساخرة', 'أدب ساخر', 'روايات', 'روايات لليافعين', 'كتب تاريخية', 'التاريخ العربي والإسلامي', 'كتب دينية', 'كتب إسلامية', 'فلسفة وتاريخ أديان', 'الحضارات القديمة', 'دواوين شعر', 'ميثولوجيا وأساطير', 'أساطير', 'دراسات تاريخية', 'تاريخ مصر', 'تاريخ أوروبا', 'نقد أدبي', 'دراسات ومقالات أدبية', 'أمثال ونوادر', 'لغات', 'اللغة العربية', 'دراسات لغوية', 'مسرحيات', 'كتب عن المسيحية', 'الخليج العربي', 'الرياضة والتسلية', 'تسلية وترفيه', 'الشرق الأوسط', 'الصحافة والإعلام', 'الصحافة', 'علم النفس وتطوير الذات', 'علم النفس', 'مقارنات أديان', 'نصوص إبداعية', 'كتب عن اليهودية', 'قانون', 'القضاء', 'رياضات بدنية', 'الترجمة', 'السفر والترحال', 'أدب الرحلات', 'تنمية بشرية وتطوير الذات', 'العادات والتقاليد', 'نثر', 'كتب سياسية', 'فكر سياسي', 'الدين والسياسة', 'مقالات سياسية', 'سياسة عربية', 'المنظمات والأحزاب', 'سياسة دولية', 'كتب قانونية', 'الصراع العربي الإسرائيلي', 'الإعلام', 'الاستشراق', 'تكنولوجيا وإنترنت', 'تكنولوجيا', 'كتب علمية', 'البيئة والطبيعة', 'العلوم', 'علم الاجتماع', 'كمبيوتر وإنترنت', 'الحيوانات', 'السيرة الذاتية والمذكرات', 'كتّاب وأدباء', 'مال وأعمال', 'إدارة الأعمال', 'التسويق', 'ريادة الأعمال', 'الاستثمار', 'الاقتصاد', 'المصارف والبنوك', 'تفسير الأحلام', 'فلاسفة ومفكرون', 'رجال دين', 'ناشطون اجتماعيون', 'فنانون', 'سياسيون', 'روّاد الأعمال', 'رياضيون', 'صحفيون', 'فلسفة', 'فكر فلسفي', 'مقالات فلسفية', 'تاريخ الفلسفة', 'ما وراء الطبيعة', 'المنطق', 'اللسانيات', 'فنون', 'مسرح', 'دراسات فنية', 'مراجع وأبحاث', 'موسوعات', 'لغات أجنبية', 'معاجم', 'أبحاث ودراسات', 'كتب أطفال', 'قصص أطفال', 'كتب تعليمية', 'موسيقى', 'الأسرة والطفل', 'ثقافة جنسية', 'العلاقة الزوجية', 'تنظيم وإدارة المنزل', 'المرأة', 'تربية الطفل', 'الطبخ', 'تصوير', 'عمارة وتصميم', 'فن إسلامي', 'تاريخ الفن', 'سينما', 'الطب والصحة', 'كتب طبية', 'الصحة البدنية', 'الأمراض والأوبئة']\n141\n\n\nfirst lets explore the top 20 genres whe have in our dataset\nEDA\n\narabic_genre = []\nfor item in all_genres:\n    arabic_genre.append(get_display(arabic_reshaper.reshape(item)))\n\n##plot genre dist\narabic_genre = nltk.FreqDist(arabic_genre) \nall_genres_df = pd.DataFrame({'Genre': list(arabic_genre.keys()), \n                              'Count': list(arabic_genre.values())})\n\ng = all_genres_df.nlargest(columns=\"Count\", n = 20)\n#plt.xlabel(arabic_reshaper.reshape(')), fontsize=18)\nplt.figure(figsize=(12,15)) \nax = sns.barplot(data=g, x= \"Count\", y = \"Genre\", palette = 'GnBu_d',edgecolor = 'w') \nax.set(ylabel = 'النوع') \nplt.show()     \nall_genres = list(dict.fromkeys(all_genres))  \n\n\n\n\n\ndf['descriptions'].head(10)\n\n0    تنقل الروايه بصدق عميق ما يدور في اعماق المراه...\n1    روايه واقعيه تركتها بين يدي المءلف امراه غريبه...\n2    بلغت بطله الروايه ايلا الزوجه التعيسه سن الارب...\n3    الارواح المتمرده هو كتاب صدر لاول مره في مدينه...\n4    الخيمياءي روايه جعلت كاتبها من اشهر الكتاب الع...\n5    في قلب حاره اليهود في الجنوب التونسي تتشابك ال...\n6    يروي جبران خليل جبران في هذا الكتاب قصه حب روح...\n7    سالتك يوم ذاك ان كنت مسترجله اذكر كيف رفعت راس...\n8    بعد خمس سنوات من العزله الاختياريه يستانف د يح...\n9    تاخذنا روايه هيبتا الي ذلك العالم الذي اهلكه ا...\nName: descriptions, dtype: object\n\n\n#show \ndef freq_words(x, terms = 30): \n   \n  all_words = ' '.join([text for text in x]) \n  all_words = all_words.split() \n\n  fdist = nltk.FreqDist(all_words) \n  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) \n  \n  # selecting top 20 most frequent words \n  d = words_df.nlargest(columns=\"count\", n = terms) \n  \n  # visualize words and frequencies\n  plt.figure(figsize=(12,15)) \n  ax = sns.barplot(data=d, x= \"count\", y = \"word\",palette = 'GnBu_d') \n  ax.set(ylabel = 'Word') \n  plt.show()\n\nfreq_words(df['descriptions'], 20)\n\n\n\n\n\ndf['descriptions'] = df['descriptions'].apply(lambda x: remove_stopwords(x))\nfreq_words(df['descriptions'], 20)\n\n\n\n\nsave the clean data set\ninvastagting the length of the our descriptions\n\ndf['description_length'] = [len(i.split()) for i in df.descriptions]\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      Book_title\n      Author\n      Cover_url\n      genres\n      descriptions\n      cover_name\n      description_length\n    \n  \n  \n    \n      0\n      أحببتك أكثر مما ينبغي\n      أثير عبد الله النشمي\n      https://cdn.abjjad.com/pub/369c6a19-3ed7-4d0f-...\n      [روايات وقصص, روايات رومانسية, روايات خيالية]\n      تنقل الروايه بصدق عميق يدور اعماق المراه حاله ...\n      369c6a19-3ed7-4d0f-a449-6f8173cf6abd-192X290.png\n      157\n    \n    \n      1\n      هكذا خُلقت\n      محمد حسين هيكل\n      https://cdn.abjjad.com/pub/cbb863f2-e58c-4fa5-...\n      [روايات وقصص, روايات اجتماعية, روايات واقعية]\n      روايه واقعيه تركتها يدي المءلف امراه غريبه غام...\n      cbb863f2-e58c-4fa5-80f3-2cba733f258f-192X290.png\n      110\n    \n    \n      2\n      قواعد العشق الأربعون\n      إليف  شافاق\n      https://cdn.abjjad.com/pub/44a22888-7eea-4714-...\n      [كتب الأدب, أدب عالمي مترجم, روايات وقصص, رواي...\n      بلغت بطله الروايه ايلا الزوجه التعيسه سن الارب...\n      44a22888-7eea-4714-9c54-3dbf463eccdd-192X290.png\n      46\n    \n  \n\n\n\n\n\nplot_data = [\n    go.Histogram(\n        x=df['description_length']\n    )\n]\nplot_layout = go.Layout(\n        title='Distribution of description length',\n        yaxis= {'title': \"Length\"},\n        xaxis= {'title': \"Descriptions\"}\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\ndef genre_count(x):\n    try:\n        return len(x)\n    except:\n        return 0\n \ndf['genre_count'] = df['genres'].map(lambda x: genre_count(x))\n#test['genre_count'] = test['genres'].map(lambda x: genre_count(x))\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Book_title\n      Author\n      Cover_url\n      genres\n      descriptions\n      cover_name\n      description_length\n      genre_count\n    \n  \n  \n    \n      0\n      أحببتك أكثر مما ينبغي\n      أثير عبد الله النشمي\n      https://cdn.abjjad.com/pub/369c6a19-3ed7-4d0f-...\n      [روايات وقصص, روايات رومانسية, روايات خيالية]\n      تنقل الروايه بصدق عميق يدور اعماق المراه حاله ...\n      369c6a19-3ed7-4d0f-a449-6f8173cf6abd-192X290.png\n      157\n      3\n    \n    \n      1\n      هكذا خُلقت\n      محمد حسين هيكل\n      https://cdn.abjjad.com/pub/cbb863f2-e58c-4fa5-...\n      [روايات وقصص, روايات اجتماعية, روايات واقعية]\n      روايه واقعيه تركتها يدي المءلف امراه غريبه غام...\n      cbb863f2-e58c-4fa5-80f3-2cba733f258f-192X290.png\n      110\n      3\n    \n    \n      2\n      قواعد العشق الأربعون\n      إليف  شافاق\n      https://cdn.abjjad.com/pub/44a22888-7eea-4714-...\n      [كتب الأدب, أدب عالمي مترجم, روايات وقصص, رواي...\n      بلغت بطله الروايه ايلا الزوجه التعيسه سن الارب...\n      44a22888-7eea-4714-9c54-3dbf463eccdd-192X290.png\n      46\n      8\n    \n    \n      3\n      الأرواح المتمردة\n      جبران خليل جبران\n      https://cdn.abjjad.com/pub/d95eb7b7-ebfd-47c1-...\n      [روايات وقصص, قصص قصيرة, روايات اجتماعية, رواي...\n      الارواح المتمرده كتاب صدر لاول مره مدينه نيويو...\n      d95eb7b7-ebfd-47c1-a1a0-7f709c45bfdc-192X290.png\n      76\n      5\n    \n    \n      4\n      الخيميائي\n      باولو كويلو\n      https://cdn.abjjad.com/pub/40cee10c-4748-4815-...\n      [كتب الأدب, أدب عالمي مترجم, روايات وقصص, رواي...\n      الخيمياءي روايه جعلت كاتبها اشهر الكتاب العالم...\n      40cee10c-4748-4815-8645-a896c6b5dd3f-192X290.png\n      127\n      8\n    \n  \n\n\n\n\n\nplot_data = [\n    go.Histogram(\n        x=df['genre_count']\n    )\n]\nplot_layout = go.Layout(\n        title='Genre distribution',\n        yaxis= {'title': \"Frequency\"},\n        xaxis= {'title': \"Number of Genres\"}\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\ndf.to_csv(r\"/Users/Lana/Desktop/Misk_DSI_capstone/abjjad.csv\",encoding=\"utf-8-sig\")"
  },
  {
    "objectID": "items/webscraping.html",
    "href": "items/webscraping.html",
    "title": "Predicting_book_genres",
    "section": "",
    "text": "for this project i webscraped abjjad using the BeautifulSoup python package\nnew_book_name =[]\nnew_author =[]\nnew_cover_url = []\nnext_page_links =[]\nnew_genres = []\nnew_descriptions = []\npage_number = 0\nfirst we get access to the website using the requests package from the main page we collect the book name, author, cover url and the link to the book detail page due to the structure of the website(no main page that contains all the books i found it easier to proccessess every genre web page separately thereby but i had to change the link multiple time\nwhile page_number<20:\n\n    result = requests.get(f'https://www.abjjad.com/books/220759001/%D8%B1%D9%88%D8%A7%D9%8A%D8%A7%D8%AA-%D9%88%D9%82%D8%B5%D8%B5/{page_number}/')       \n    src = result.content\n\n\n    goodsoup = BeautifulSoup(src,\"lxml\")\n\n     ## book title\n    book_name = goodsoup.find_all(\"a\", {\"data-ga\":\"BookBadge_Title\"})\n    \n    ## author name\n    author = goodsoup.find_all('span',{\"class\":\"author\"})\n \n    cover_url = goodsoup.find_all(\"a\",{\"class\":\"img\"})\n\n    #saving the detail page link into an array to get acceses to more data \n    for i in range(len(book_name)):\n        next_page_links.append(\"https://www.abjjad.com\" + str(book_name[i]).split('href=\"')[1].lstrip().split('\">')[0]) \n\n    for i in range(len(book_name)):        \n        NremovedLinks = str(book_name[i]).split('>')[1].lstrip().split('<')[0]\n        NremovedR = NremovedLinks.replace('\\r', '')\n        new_book_name.append(NremovedR.replace('\\n',''))\n\n        AremovedLinks = str(author[i].text)\n        AremovedR = AremovedLinks.replace('\\r', '')\n        new_author.append(AremovedR.replace('\\n',''))\n        new_cover_url.append(str(cover_url[i]).split('src=\"')[1].lstrip().split('\"/>')[0])\nthis loop gets the descriptions and genres from the book detail page\n\nfor link in next_page_links:\n    \n    secondresult = requests.get(link)\n    secondsrc = secondresult.content\n    secondgoodsoup = BeautifulSoup(secondsrc,'lxml') \n    genres = secondgoodsoup.find(\"ul\", {\"itemprop\":\"genre\" })\n    output_text = \"\"\n    for item in genres.find_all('li'): \n        output_text += item.text+\"|\"\n       \n    new_genres.append(output_text)\n\n    desc = secondgoodsoup.find(\"span\" ,{\"itemprop\":\"description\" , \"class\":\"content\"})\n    DremovedLinks = str(desc).split('>')[1].lstrip().split('<')[0]\n    DremovedR = DremovedLinks.replace('\\r', '')\n    new_descriptions.append(DremovedR.replace('\\n',''))\n    \nasave the collected dataset to CSV file\nfile_list = [new_book_name,new_author,new_cover_url,new_genres,new_descriptions]\nexpo = zip_longest(*file_list)\nwith open(\"C:\\\\Users\\\\Lana\\\\Desktop\\\\Misk_DSI_capstone\\\\data\\\\abjjad.csv\" , \"w\" , encoding=\"utf-8-sig\") as myfile:\n    writerobj = csv.writer(myfile)\n    writerobj.writerow([\"Book_title\",\"Author\",\"Cover_url\",\"genres\",\"descriptions\"])\n    writerobj.writerows(expo)"
  },
  {
    "objectID": "items/Model.html",
    "href": "items/Model.html",
    "title": "Predicting_book_genres",
    "section": "",
    "text": "df = pd.read_csv(\"/Users/Lana/Desktop/Misk_DSI_capstone/abjjad.csv\",encoding=\"utf-8-sig\")\n\ndf.head(4)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Book_title\n      Author\n      Cover_url\n      genres\n      descriptions\n      cover_name\n      description_length\n      genre_count\n    \n  \n  \n    \n      0\n      0\n      أحببتك أكثر مما ينبغي\n      أثير عبد الله النشمي\n      https://cdn.abjjad.com/pub/369c6a19-3ed7-4d0f-...\n      ['روايات وقصص', 'روايات رومانسية', 'روايات خيا...\n      تنقل الروايه بصدق عميق يدور اعماق المراه حاله ...\n      369c6a19-3ed7-4d0f-a449-6f8173cf6abd-192X290.png\n      157\n      3\n    \n    \n      1\n      1\n      هكذا خُلقت\n      محمد حسين هيكل\n      https://cdn.abjjad.com/pub/cbb863f2-e58c-4fa5-...\n      ['روايات وقصص', 'روايات اجتماعية', 'روايات واق...\n      روايه واقعيه تركتها يدي المءلف امراه غريبه غام...\n      cbb863f2-e58c-4fa5-80f3-2cba733f258f-192X290.png\n      110\n      3\n    \n    \n      2\n      2\n      قواعد العشق الأربعون\n      إليف  شافاق\n      https://cdn.abjjad.com/pub/44a22888-7eea-4714-...\n      ['كتب الأدب', 'أدب عالمي مترجم', 'روايات وقصص'...\n      بلغت بطله الروايه ايلا الزوجه التعيسه سن الارب...\n      44a22888-7eea-4714-9c54-3dbf463eccdd-192X290.png\n      46\n      8\n    \n    \n      3\n      3\n      الأرواح المتمردة\n      جبران خليل جبران\n      https://cdn.abjjad.com/pub/d95eb7b7-ebfd-47c1-...\n      ['روايات وقصص', 'قصص قصيرة', 'روايات اجتماعية'...\n      الارواح المتمرده كتاب صدر لاول مره مدينه نيويو...\n      d95eb7b7-ebfd-47c1-a1a0-7f709c45bfdc-192X290.png\n      76\n      5\n    \n  \n\n\n\n\n##Converting genres to Features\nmultilabel_binarizer = MultiLabelBinarizer()\nmultilabel_binarizer.fit(df['genres'])\n\ny = multilabel_binarizer.transform(df['genres'])\n\nvect = TfidfVectorizer(max_df=0.8, max_features=4000)\n# transform target variable\n#y = multilabel_binarizer.transform(D_FMT['genres'])\n\n# split dataset \nxtrain, xtest, ytrain, ytest = train_test_split(df['descriptions'], y, test_size=0.2, random_state=9)\n\n# TF-IDF \nxtrain_tfidf = vect.fit_transform(xtrain)\nxtest_tfidf = vect.transform(xtest)\n\n## MODEL\nlr = LogisticRegression()\nclf = OneVsRestClassifier(lr)\n\n# fit model \nclf.fit(xtrain_tfidf, ytrain)\n\n# make predictions for validation set\ny_pred = clf.predict(xtest_tfidf)\nprint(y_pred[5])\n\nprint(multilabel_binarizer.inverse_transform(y_pred)[3])\n\ny_pred_prob = clf.predict_proba(xtest_tfidf)\n\n\nthreshold = 0.15 # threshold value\ny_pred_new = (y_pred_prob >= threshold).astype(int)\n\n# evaluate performance\nprint(f1_score(ytest, y_pred_new, average=\"micro\"))\nprint(accuracy_score(ytest, y_pred_new))"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Predicting book genres",
    "section": "Background",
    "text": "Background\nwhen working with a large amount of books reading every book and trying to figure out what genre it is can be a time consuming task thats why we need to find a way to automate the process of genre labeling."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Predicting book genres",
    "section": "Data",
    "text": "Data\nthe dataset was obtained by webscraping “abjjed” using the beautifulsoup python package\n\n\n\nattribute\ndescription\n\n\n\n\nBook_title\ntitle of the book\n\n\nAuthor\nauthor name\n\n\ncover_url\nbook cover image url\n\n\ngenres\nlist of genres that a book falls under\n\n\ndescriptions\nbook description\n\n\ncover_name\nstring name of the cover image\n\n\ndescription_length\nthe word count of the description\n\n\ngenre_count\nnumber of genres that a book has"
  }
]